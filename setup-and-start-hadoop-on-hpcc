#!/bin/bash

if [ -z "$MINGENV_HADOOP" ]; then
  echo "Please source the setup file before trying to start hadoop"
  exit 1
fi

# if JAVA_HOME is not defined try to define it
if [ -z "$JAVA_HOME" ]; then
  export JAVA_HOME=/usr/lib/jvm/java
  echo "no JAVA_HOME, set as $JAVA_HOME"
fi

HADOOP_TEMPLATE_DIR=${HADOOP_TEMPLATE_DIR:-/path/to/configuration/dir}

# export HADOOP_HOME=/usr/usc/hadoop/default
export HADOOP_HOME=/path/to/hadoop/home/directory/
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin:${JAVA_HOME}/bin

# These vars can be set for development purposes but default to production values
HADOOP_LOG_DIR=${HADOOP_LOG_DIR:-$TMPDIR/hadoop/logs}
HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-/scratch/hadoop/conf}

# These could be set by the user if needed
HADOOP_MAP_TASK_HEAP=${HADOOP_MAP_TASK_HEAP:-1024}
HADOOP_REDUCE_TASK_HEAP=${HADOOP_REDUCE_TASK_HEAP:-1024}


MAPRED_HOSTS=${HADOOP_CONF_DIR}/slaves
max_map_tasks=${HADOOP_MAX_MAP_TASKS:-$PBS_NUM_PPN}
max_reduce_tasks=${HADOOP_MAX_REDUCE_TASKS:-$PBS_NUM_PPN}


# export ISOLATE_JOB_CLIENT and ISOLATE_JOB_TRACKER to be yes if you
# want them on nodes which have no mapreduce task processes
isolate_jobclient=${HADOOP_ISOLATE_JOB_CLIENT:-no}
isolate_jobtracker=${HADOOP_ISOLATE_JOB_TRACKER:=no}

# Configure the nodes and launch HDFS and YARN among nodes
# Distributed the configuration files through scratch file system, OrangeFS.
if [ -d /scratch ]; then
  echo $(date) - Setting up Hadoop Cluster
  echo Hadoop configuration is in ${HADOOP_CONF_DIR}
  echo Hadoop logs are in ${HADOOP_LOG_DIR} 

  mkdir -p $HADOOP_CONF_DIR
  
  # Create hadoop config for a job on the USC HPCC cluster
  /usr/bin/rsync -a $HADOOP_TEMPLATE_DIR/ $HADOOP_CONF_DIR/

  # Setup the slaves file to be the hostnames as the NodeManagers or DataNodes
  for nodeManager in $(cat $PBS_NODEFILE | sort -u); do  
    getent hosts $nodeManager | awk '{print $2}' 
  done > ${MAPRED_HOSTS}

  # Create local temp directories on the task nodes
  for nodeManager in $(cat ${MAPRED_HOSTS}); do  
    ssh $nodeManager "mkdir -p $HADOOP_LOG_DIR"
  done

  # Modify the MAPRED_HOSTS file to remove the SUBMISSION_NODE and MASTER entries.  The first entry is for the user's
  # hadoop job submission process and the last entry is for the job tracker. This is done to give more memory to 
  # all of the hadoop processes.
  SUBMISSION_NODE=$(getent hosts $(/bin/hostname) | awk '{print $2}')
  if [ $isolate_jobclient != "no" ]; then
    sed -i -e "/$SUBMISSION_NODE/d" ${MAPRED_HOSTS}
  fi
  MASTER=$(head -n 1 ${MAPRED_HOSTS})
  if [ $isolate_jobtracker != "no" ]; then
    sed -i -e "/$MASTER/d" ${MAPRED_HOSTS}
  fi 
  # Modify the master file to let all the nodes know the hostname of NameNode and ResourceManager
  echo $MASTER > ${HADOOP_CONF_DIR}/masters
  
  # Randomly (the last node in the list) chosen a node to be the secondary NameNode
  SECONDARYNAMENODE=$(sed -n '$p' ${MAPRED_HOSTS})

  # We need at least 1 mapred task host
  cat ${MAPRED_HOSTS}
  if [ ! $(cat ${MAPRED_HOSTS} | wc -l) -gt 0 ]; then
    echo "ERROR: at least 1 node is needed to run hadoop map reduce tasks.  Please increase the value of nodes= in your qsub parameter."
    exit 1
  fi

  mapred_hosts=$(echo $MAPRED_HOSTS | sed 's/\//\\\//g')
  sed -i -e "s/#MASTER#/$MASTER/g" \
         -e "s/#MAPREDHOSTS#/$mapred_hosts/g" \
         -e "s/#MAPTASKS#/$max_map_tasks/g" \
         -e "s/#REDUCETASKS#/$max_reduce_tasks/g" \
         -e "s/#MAPTASKHEAP#/$HADOOP_MAP_TASK_HEAP/g" \
         -e "s/#REDUCETASKHEAP#/$HADOOP_REDUCE_TASK_HEAP/g" \
         ${HADOOP_CONF_DIR}/mapred-site.xml 

  # Setup the $TMPDIR to be the base of all the Hadoop operations
  tmp_dir=$TMPDIR/hadoop
  tmp_dir=$(echo $tmp_dir | sed 's/\//\\\//g')
  sed -i -e "s/#MASTER#/$MASTER/g" \
         -e "s/#TMPDIR#/$tmp_dir/g" \
         ${HADOOP_CONF_DIR}/core-site.xml 

  # Define the hostname of the SourceManager
  sed -i -e "s/#MASTER#/$MASTER/g" \
         ${HADOOP_CONF_DIR}/yarn-site.xml

  # Define the hostname of the Secondary NameNode
  sed -i -e "s/#SECONDARY_HTTP#/$SECONDARYNAMENODE/g" \
       ${HADOOP_CONF_DIR}/hdfs-site.xml

  # Setup the required environment variables in the launch up script of Hadoop 
  echo "export HADOOP_LOG_DIR=${HADOOP_LOG_DIR}" >> ${HADOOP_CONF_DIR}/hadoop-env.sh
  echo "export PATH=${PATH}" >> ${HADOOP_CONF_DIR}/hadoop-env.sh
  echo "export JAVA_HOME=${JAVA_HOME}" >> ${HADOOP_CONF_DIR}/hadoop-env.sh

  # 1. Format the NameNode
  # 2. Start the NameNode, DataNodes, Secondary NameNode
  # 3. Start the Source Manager and Node Managers
  ssh $MASTER "${HADOOP_HOME}/bin/hdfs namenode -format"
  ssh $MASTER "HADOOP_HEAP=$HADOOP_HEAP; ${HADOOP_HOME}/sbin/start-dfs.sh"
  ssh $MASTER "HADOOP_HEAP=$HADOOP_HEAP; ${HADOOP_HOME}/sbin/start-yarn.sh"

else
  echo "ERROR: The /scratch filesystem does not exist.  Unable to run hadoop jobs without /scratch"
  exit 1
fi
